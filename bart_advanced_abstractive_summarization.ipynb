{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if running in Colab)\n",
    "%pip install --upgrade torch transformers datasets pandas --quiet\n",
    "%pip install \"transformers[torch]\" \"accelerate>=0.26.0\"\n",
    "%pip install matplotlib seaborn\n",
    "%pip install protobuf\n",
    "%pip install rouge-score matplotlib-venn nltk\n",
    "%pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import zipfile\n",
    "\n",
    "# File paths for the downloaded files (update with actual paths if needed)\n",
    "train_zip_path = \"/Users/sajinshrestha/Developer/college/text_summarizer/datasets/train.csv.zip\"\n",
    "val_csv_path = \"/Users/sajinshrestha/Developer/college/text_summarizer/datasets/val.csv\"\n",
    "test_csv_path = \"/Users/sajinshrestha/Developer/college/text_summarizer/datasets/test.csv\"\n",
    "\n",
    "# Directory to extract train.csv from train.csv.zip\n",
    "extract_dir = \"./extracted_datasets\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Unzip train.csv.zip\n",
    "with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "print(f\"Extracted train.csv.zip to {extract_dir}\")\n",
    "\n",
    "# Find the extracted train.csv\n",
    "extracted_files = os.listdir(extract_dir)\n",
    "train_csv_path = None\n",
    "for file in extracted_files:\n",
    "    if file.lower() == 'train.csv':\n",
    "        train_csv_path = os.path.join(extract_dir, file)\n",
    "        break\n",
    "if train_csv_path is None:\n",
    "    raise FileNotFoundError(f\"train.csv not found in {extract_dir}. Extracted files: {extracted_files}\")\n",
    "\n",
    "# Load CSVs using pandas to avoid filesystem issues\n",
    "try:\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    validation_df = pd.read_csv(val_csv_path)\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV files: {e}\")\n",
    "    raise\n",
    "\n",
    "# Convert pandas DataFrames to HuggingFace Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Print column names for each dataset\n",
    "print(\"\\nColumn names in Train Dataset:\", train_dataset.column_names)\n",
    "print(\"Column names in Validation Dataset:\", validation_dataset.column_names)\n",
    "print(\"Column names in Test Dataset:\", test_dataset.column_names)\n",
    "\n",
    "# Preview the datasets\n",
    "print(f\"\\nTrain data (total {len(train_dataset)} rows)\")\n",
    "print(f\"\\nValidation data (total {len(validation_dataset)} rows)\")\n",
    "print(f\"\\nTest data (total {len(test_dataset)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f0ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install \"transformers[torch]\" \"accelerate>=0.26.0\"\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# --- Data Cleaning Function ---\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\s+', ' ', text)   # Normalize whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)  # Keep alphanumeric and select punctuation\n",
    "    return text.strip()\n",
    "\n",
    "# --- Load and Prepare Datasets ---\n",
    "extract_dir = \"/content/extracted_datasets\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "with zipfile.ZipFile(\"/content/train.csv.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "train_csv_path = os.path.join(extract_dir, 'train.csv')\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path).dropna(subset=['document', 'summary']).astype(str)\n",
    "validation_df = pd.read_csv(\"/content/val.csv\").dropna(subset=['document', 'summary']).astype(str)\n",
    "\n",
    "# Clean and filter data\n",
    "for df in [train_df, validation_df]:\n",
    "    df['document'] = df['document'].apply(clean_text)\n",
    "    df['summary'] = df['summary'].apply(clean_text)\n",
    "    df.drop(df[df['document'].str.len() < 50].index, inplace=True)\n",
    "    df.drop(df[df['summary'].str.len() < 10].index, inplace=True)\n",
    "\n",
    "# Load tokenizer for filtering\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Filter by token length\n",
    "train_df = train_df[train_df['summary'].apply(lambda x: 15 < len(tokenizer(x, add_special_tokens=True)['input_ids']) <= 120)]\n",
    "validation_df = validation_df[validation_df['summary'].apply(lambda x: 15 < len(tokenizer(x, add_special_tokens=True)['input_ids']) <= 120)]\n",
    "\n",
    "# Shuffle training data\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "\n",
    "# --- Preprocessing function ---\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples['document'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        examples['summary'],\n",
    "        max_length=100,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    labels = outputs['input_ids']\n",
    "    labels = [[(label if label != tokenizer.pad_token_id else -100) for label in labels_seq] for labels_seq in labels]\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=['document', 'summary'])\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True, remove_columns=['document', 'summary'])\n",
    "\n",
    "# --- Load Model ---\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "model.config.label_smoothing_factor = 0.1\n",
    "model.config.dropout = 0.3\n",
    "model.config.attention_dropout = 0.2\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "\n",
    "# --- Training arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/results_optimized',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_strategy='epoch',  # Evaluate at the end of each epoch\n",
    "    save_strategy='epoch',  # Save at the end of each epoch\n",
    "    logging_strategy='epoch',  # Log training loss at the end of each epoch\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=100,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True,\n",
    "    weight_decay=0.1,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type='cosine',\n",
    "    report_to='none',\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "trainer.train()\n",
    "\n",
    "# --- Save Model and Tokenizer ---\n",
    "model.save_pretrained('/content/results_optimized/finetuned_bart_large_cnn')\n",
    "tokenizer.save_pretrained('/content/results_optimized/finetuned_bart_large_cnn')\n",
    "\n",
    "# --- Extract and Save Metrics ---\n",
    "metrics = []\n",
    "for log in trainer.state.log_history:\n",
    "    epoch = log.get('epoch', None)\n",
    "    train_loss = log.get('loss', None)\n",
    "    eval_loss = log.get('eval_loss', None)\n",
    "    if epoch is not None and (train_loss is not None or eval_loss is not None):\n",
    "        metrics.append({'epoch': epoch, 'train_loss': train_loss, 'eval_loss': eval_loss})\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df.to_csv('training_metrics.csv', index=False)\n",
    "print(\"\\nTraining and Validation Loss Metrics (per epoch):\")\n",
    "print(metrics_df)\n",
    "\n",
    "print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a31df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wordcloud\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from matplotlib_venn import venn2\n",
    "from wordcloud import WordCloud\n",
    "import uuid\n",
    "\n",
    "# Set environment variable to avoid distributed training issues\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Create output directory for saving metrics\n",
    "output_dir = \"summarization_metrics\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Sample data for text summarization\n",
    "# data = [\n",
    "#     {\n",
    "#         'document': (\n",
    "#             \"A massive storm swept through the eastern coast on Friday, causing widespread power outages \"\n",
    "#             \"and flooding in several towns. Emergency services responded quickly, evacuating residents \"\n",
    "#             \"from high-risk areas. Meteorologists warned that more heavy rainfall is expected over the weekend.\"\n",
    "#         ),\n",
    "#         'summary': (\n",
    "#             \"A storm hit the eastern coast, causing floods and outages, with more rain expected.\"\n",
    "#         )\n",
    "#     }\n",
    "# ]\n",
    "# data = [\n",
    "#     {\n",
    "#         'document': (\n",
    "#             \"The city council approved a new public transportation initiative on Tuesday, aiming to reduce traffic congestion \"\n",
    "#             \"and carbon emissions. The plan includes expanding bus routes, introducing electric buses, and offering discounted \"\n",
    "#             \"fares for students and seniors. Officials believe the project will significantly improve urban mobility.\"\n",
    "#         ),\n",
    "#         'summary': (\n",
    "#             \"A new transit plan with electric buses and discounts was approved to ease traffic and cut emissions.\"\n",
    "#         )\n",
    "#     }\n",
    "# ]\n",
    "# data = [\n",
    "#     {\n",
    "#         'document': (\n",
    "#             \"Lena had always dreamed of becoming a painter, but life in her small village offered few opportunities. \"\n",
    "#             \"One day, a traveling art teacher visited the local school and noticed Lena’s sketches. Impressed by her talent, \"\n",
    "#             \"he offered to mentor her. Years later, Lena's artwork would be displayed in galleries across the country.\"\n",
    "#         ),\n",
    "#         'summary': (\n",
    "#             \"A village girl’s painting talent is discovered by a mentor, leading her to national art success.\"\n",
    "#         )\n",
    "#     }\n",
    "# ]\n",
    "# data = [\n",
    "#     {\n",
    "#         'document': (\n",
    "#             \"Regular physical activity has been linked to numerous health benefits, including reduced risk of chronic diseases such as heart disease, diabetes, and obesity.\" \n",
    "#             \" Experts recommend at least 150 minutes of moderate exercise per week.\" \n",
    "#             \" In addition to physical health, exercise also improves mental well-being by reducing stress and boosting mood.\"\n",
    "#         ),\n",
    "#         'summary': (\n",
    "#             \"Exercise lowers disease risk and boosts mental health; experts advise 150 minutes weekly activity.\"\n",
    "#         )\n",
    "#     }\n",
    "# ]\n",
    "data = [\n",
    "    {\n",
    "        'document': (\n",
    "            \"Marie Curie was a pioneering physicist and chemist who conducted groundbreaking research on radioactivity. \"\n",
    "            \"She was the first woman to win a Nobel Prize and remains the only person to win Nobel Prizes in two different sciences — Physics and Chemistry. \"\n",
    "            \"Curie's discoveries led to major advancements in medical treatments, especially in cancer therapy. \"\n",
    "            \"Despite working in hazardous conditions, she remained committed to science until her death in 1934 from radiation exposure.\"\n",
    "        ),\n",
    "        'summary': (\n",
    "            \"Marie Curie, a Nobel-winning scientist, made key discoveries in radioactivity that advanced cancer treatment.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert sample data to a HuggingFace Dataset\n",
    "try:\n",
    "    sample_df = pd.DataFrame(data)\n",
    "    sample_dataset = Dataset.from_pandas(sample_df)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating dataset: {e}\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_dir = \"/Users/sajinshrestha/Developer/college/text_summarizer/custom-trained-modal-final\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_dir, local_files_only=True)\n",
    "\n",
    "# Clean and post-process the predicted summary\n",
    "def clean_summary(text):\n",
    "    # Split into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    # Remove duplicate sentences and incomplete sentences\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    for s in sentences:\n",
    "        s_strip = s.strip()\n",
    "        if s_strip and s_strip not in seen and re.match(r'.*[.!?]$', s_strip):\n",
    "            cleaned.append(s_strip)\n",
    "            seen.add(s_strip)\n",
    "    # If last sentence is incomplete, add a period\n",
    "    if cleaned and not cleaned[-1].endswith('.'):\n",
    "        cleaned[-1] += '.'\n",
    "    # Join sentences and normalize whitespace\n",
    "    result = ' '.join(cleaned)\n",
    "    result = re.sub(r'\\.\\.+', '.', result)\n",
    "    result = re.sub(r'\\s+', ' ', result).strip()\n",
    "    return result\n",
    "\n",
    "# Perform summarization\n",
    "input_text = sample_dataset['document'][0]\n",
    "if not input_text.strip():\n",
    "    raise ValueError(\"Input document is empty.\")\n",
    "inputs = tokenizer([input_text], max_length=512, return_tensors='pt', truncation=True, padding=True)\n",
    "try:\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_beams=6,\n",
    "        max_length=128,\n",
    "        min_length=40,\n",
    "        length_penalty=1.0,  \n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    predicted_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    predicted_summary = clean_summary(predicted_summary)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error during summarization: {e}\")\n",
    "\n",
    "actual_summary = sample_dataset[\"summary\"][0]\n",
    "\n",
    "# Print summaries for reference\n",
    "print(\"\\n=== Text Summarization Results ===\")\n",
    "print(f\"Original Document: {input_text}\\n\")\n",
    "print(f\"Predicted Summary: {predicted_summary}\\n\")\n",
    "print(f\"Actual Summary: {actual_summary}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize metrics dictionary\n",
    "metrics = {}\n",
    "\n",
    "# 1. Summary Length Comparison\n",
    "pred_len = len(predicted_summary.split())\n",
    "actual_len = len(actual_summary.split())\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=['Predicted', 'Actual'], y=[pred_len, actual_len], palette='viridis')\n",
    "plt.title('Summary Length Comparison (Word Count)', fontsize=14)\n",
    "plt.ylabel('Number of Words', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'summary_length_comparison.png'), dpi=300)\n",
    "plt.show()\n",
    "metrics['Length_Predicted'] = pred_len\n",
    "metrics['Length_Actual'] = actual_len\n",
    "print(\"Bar chart comparing the word count of predicted and actual summaries.\")\n",
    "print(f\"Description: This chart compares the number of words in the predicted summary ({pred_len}) and the actual summary ({actual_len}). Ideally, the predicted summary should be concise but not too short compared to the reference. If the predicted summary is much shorter, it may miss key information; if much longer, it may be verbose or off-topic.\")\n",
    "\n",
    "# 2. Cosine Similarity (TF-based)\n",
    "vectorizer = CountVectorizer().fit([predicted_summary, actual_summary])\n",
    "vectors = vectorizer.transform([predicted_summary, actual_summary]).toarray()\n",
    "cos_sim = cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.barplot(x=['Cosine Similarity'], y=[cos_sim], palette='viridis')\n",
    "plt.title('Cosine Similarity (Predicted vs Actual)', fontsize=14)\n",
    "plt.ylabel('Similarity Score', fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'cosine_similarity.png'), dpi=300)\n",
    "plt.show()\n",
    "metrics['Cosine_Similarity'] = cos_sim\n",
    "print(\"Bar chart showing cosine similarity between predicted and actual summaries.\")\n",
    "print(f\"Description: Cosine similarity measures the overlap in word usage between summaries. A value of {cos_sim:.3f} (range 0-1) means the summaries are {cos_sim*100:.1f}% similar in terms of word distribution. Values above 0.7 are considered strong; below 0.5 may indicate poor relevance.\")\n",
    "\n",
    "# 3. ROUGE Scores\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = scorer.score(actual_summary, predicted_summary)\n",
    "rouge1 = rouge_scores['rouge1'].fmeasure\n",
    "rouge2 = rouge_scores['rouge2'].fmeasure\n",
    "rougeL = rouge_scores['rougeL'].fmeasure\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=['ROUGE-1', 'ROUGE-2', 'ROUGE-L'], y=[rouge1, rouge2, rougeL], palette='viridis')\n",
    "plt.title('ROUGE Scores (F1 Measure)', fontsize=14)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'rouge_scores.png'), dpi=300)\n",
    "plt.show()\n",
    "metrics['ROUGE-1'] = rouge1\n",
    "metrics['ROUGE-2'] = rouge2\n",
    "metrics['ROUGE-L'] = rougeL\n",
    "print(\"Bar chart showing ROUGE scores (F1) for n-gram overlap between summaries.\")\n",
    "print(f\"Description: ROUGE-1 ({rouge1:.3f}) measures unigram overlap, ROUGE-2 ({rouge2:.3f}) measures bigram overlap, and ROUGE-L ({rougeL:.3f}) measures longest common subsequence. High values (above 0.7) indicate the predicted summary captures key phrases and structure from the reference.\")\n",
    "\n",
    "# 4. BLEU Score\n",
    "reference = [nltk.word_tokenize(actual_summary)]\n",
    "candidate = nltk.word_tokenize(predicted_summary)\n",
    "bleu_score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.barplot(x=['BLEU Score'], y=[bleu_score], palette='viridis')\n",
    "plt.title('BLEU Score', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'bleu_score.png'), dpi=300)\n",
    "plt.show()\n",
    "metrics['BLEU'] = bleu_score\n",
    "print(\"Bar chart showing BLEU score for sequence similarity.\")\n",
    "print(f\"Description: BLEU score ({bleu_score:.3f}) evaluates n-gram precision. Scores above 0.5 are good for summarization. Low BLEU may mean the summary uses different wording or order than the reference.\")\n",
    "\n",
    "# 5. Word Overlap Venn Diagram\n",
    "pred_words = set(predicted_summary.lower().split())\n",
    "actual_words = set(actual_summary.lower().split())\n",
    "plt.figure(figsize=(6, 6))\n",
    "venn2([actual_words, pred_words], set_labels=('Actual', 'Predicted'), set_colors=('#1f77b4', '#ff7f0e'))\n",
    "plt.title('Word Overlap Venn Diagram', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'word_overlap_venn.png'), dpi=300)\n",
    "plt.show()\n",
    "overlap = len(pred_words & actual_words)\n",
    "total = len(actual_words)\n",
    "metrics['Word_Overlap_Ratio'] = overlap / total if total > 0 else 0\n",
    "print(\"Venn diagram visualizing word overlap between predicted and actual summaries.\")\n",
    "print(f\"Description: {overlap} words overlap out of {total} actual summary words. High overlap means the summary is relevant, but excessive overlap may mean copying instead of abstraction. A good summary balances overlap and novelty.\")\n",
    "\n",
    "# 6. Pie Chart of Overlap vs Non-Overlap\n",
    "non_overlap = len((pred_words | actual_words) - (pred_words & actual_words))\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie([overlap, non_overlap], labels=['Overlap', 'Non-Overlap'], autopct='%1.1f%%', colors=['#1f77b4', '#ff7f0e'])\n",
    "plt.title('Word Overlap vs Non-Overlap', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'overlap_pie_chart.png'), dpi=300)\n",
    "plt.show()\n",
    "print(\"Pie chart showing proportion of overlapping and non-overlapping words.\")\n",
    "print(f\"Description: {overlap/(overlap+non_overlap):.2%} of words overlap. A balanced ratio (40-70%) is ideal. Too high means copying, too low means poor relevance.\")\n",
    "\n",
    "# 7. Word Frequency Distribution\n",
    "all_words = nltk.word_tokenize(predicted_summary + ' ' + actual_summary)\n",
    "word_freq = Counter(all_words)\n",
    "most_common = word_freq.most_common(10)\n",
    "words, freqs = zip(*most_common) if most_common else ([], [])\n",
    "if words:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=list(words), y=list(freqs), palette='viridis')\n",
    "    plt.title('Top 10 Word Frequency Distribution', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'word_frequency.png'), dpi=300)\n",
    "    plt.show()\n",
    "print(\"Bar chart showing the most frequent words in both summaries.\")\n",
    "print(f\"Description: Frequent words ({', '.join(words)}) should be key concepts. If stopwords dominate, the summary may lack substance.\")\n",
    "\n",
    "# 8. Word Cloud for Predicted and Actual Summaries\n",
    "combined_text = predicted_summary + ' ' + actual_summary\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(combined_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Predicted and Actual Summaries', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'word_cloud.png'), dpi=300)\n",
    "plt.show()\n",
    "print(\"Word cloud visualizing the most prominent words in both summaries.\")\n",
    "print(\"Description: Larger words are more frequent. Good summaries highlight key concepts, not just repeated words. If irrelevant words are large, the summary may be off-topic.\")\n",
    "\n",
    "# 9. Token-level Precision, Recall, F1, Accuracy\n",
    "pred_tokens = predicted_summary.split()\n",
    "actual_tokens = actual_summary.split()\n",
    "all_tokens = list(set(pred_tokens + actual_tokens))\n",
    "pred_bin = [1 if t in pred_tokens else 0 for t in all_tokens]\n",
    "actual_bin = [1 if t in actual_tokens else 0 for t in all_tokens]\n",
    "precision = precision_score(actual_bin, pred_bin)\n",
    "recall = recall_score(actual_bin, pred_bin)\n",
    "f1 = f1_score(actual_bin, pred_bin)\n",
    "accuracy = accuracy_score(actual_bin, pred_bin)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=['Precision', 'Recall', 'F1', 'Accuracy'], y=[precision, recall, f1, accuracy], palette='viridis')\n",
    "plt.title('Token-level Precision, Recall, F1, Accuracy', fontsize=14)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'token_metrics.png'), dpi=300)\n",
    "plt.show()\n",
    "metrics['Precision'] = precision\n",
    "metrics['Recall'] = recall\n",
    "metrics['F1'] = f1\n",
    "metrics['Accuracy'] = accuracy\n",
    "print(\"Bar chart showing token-level precision, recall, F1, and accuracy.\")\n",
    "print(f\"Description: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}, Accuracy={accuracy:.3f}. Values above 0.7 are good. Low values mean the model misses or mispredicts key tokens. Precision is the fraction of predicted tokens that are correct; recall is the fraction of actual tokens that are found; F1 balances both.\")\n",
    "\n",
    "# 10. Confusion Matrix\n",
    "cm = confusion_matrix(actual_bin, pred_bin)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not in Pred', 'In Pred'], yticklabels=['Not in Actual', 'In Actual'])\n",
    "plt.title('Token-level Confusion Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'), dpi=300)\n",
    "plt.show()\n",
    "print(\"Heatmap showing the confusion matrix for token-level prediction.\")\n",
    "print(f\"Description: The confusion matrix shows true positives (bottom-right), true negatives (top-left), false positives (top-right), and false negatives (bottom-left). High true positives/negatives are good; high false values mean the model is missing or mispredicting tokens.\")\n",
    "\n",
    "# 11. Sentence Length Difference (in tokens)\n",
    "pred_sent_len = [len(nltk.word_tokenize(sent)) for sent in nltk.sent_tokenize(predicted_summary)]\n",
    "actual_sent_len = [len(nltk.word_tokenize(sent)) for sent in nltk.sent_tokenize(actual_summary)]\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=[pred_sent_len, actual_sent_len], palette='viridis')\n",
    "plt.xticks([0, 1], ['Predicted', 'Actual'])\n",
    "plt.title('Sentence Length Distribution (Tokens)', fontsize=14)\n",
    "plt.ylabel('Tokens per Sentence', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'sentence_length_boxplot.png'), dpi=300)\n",
    "plt.show()\n",
    "print(\"Boxplot comparing sentence length distribution in predicted and actual summaries.\")\n",
    "print(f\"Description: Predicted sentences have lengths: {pred_sent_len}. Actual: {actual_sent_len}. Ideally, sentence lengths should be similar, indicating comparable structure. Large differences may mean poor coherence or structure.\")\n",
    "\n",
    "# 12. Metrics Summary Table\n",
    "metrics_table = pd.DataFrame({\n",
    "    'Metric': list(metrics.keys()),\n",
    "    'Score': list(metrics.values())\n",
    "}).round(3)\n",
    "print(\"\\n=== Metrics Summary ===\")\n",
    "print(metrics_table)\n",
    "metrics_table.to_csv(os.path.join(output_dir, 'metrics_summary.csv'), index=False)\n",
    "print(\"Table summarizing all calculated metrics.\")\n",
    "print(\"Description: This table aggregates all metrics for a holistic view. High scores across metrics indicate a strong summarizer. Review for consistency and outliers.\")\n",
    "\n",
    "# 13. Classification Report\n",
    "print(\"\\n=== Token-level Classification Report ===\")\n",
    "print(classification_report(actual_bin, pred_bin, target_names=['Not in Summary', 'In Summary']))\n",
    "print(\"Detailed classification report for token-level prediction. High precision and recall are desired. Look for balanced support and low false positives/negatives.\")\n",
    "\n",
    "# --- Status Analysis ---\n",
    "print(\"\\n=== Status Analysis ===\")\n",
    "if cos_sim > 0.7 and rouge1 > 0.7 and bleu_score > 0.5 and f1 > 0.7:\n",
    "    print(\"Status: The summarization model is performing well. Most metrics are in the good range. Summaries are relevant, concise, and well-structured. No major issues detected.\")\n",
    "elif cos_sim > 0.5 and rouge1 > 0.5 and f1 > 0.5:\n",
    "    print(\"Status: The model is moderately effective. Some metrics are below optimal. Consider tuning model parameters, increasing training data, or improving preprocessing for better abstraction and token prediction.\")\n",
    "else:\n",
    "    print(\"Status: The model needs improvement. Metrics are below recommended thresholds. Consider more training, data cleaning, or model architecture changes. Review individual diagrams for specific weaknesses.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
